{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Multimodal Classifier\n",
    "\n",
    "Ce notebook construit un modèle de classification du cancer du sein en TensorFlow en combinant :\n",
    "- des images 4 canaux (mammographies Gauche/Droite, LE/SUB),\n",
    "- des métadonnées cliniques (Excel),\n",
    "- des rapports médicaux (texte).\n",
    "\n",
    "**Variable cible : `Pathology`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Concatenate, Embedding, Bidirectional, GRU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Charger et préparer les données (images + Excel + JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📁 Dossiers (à adapter selon ton organisation)\n",
    "image_dir_le = 'CDD-CESM/PKG - CDD-CESM/CDD-CESM/Low energy images of CDD-CESM'\n",
    "image_dir_sub = 'CDD-CESM/PKG - CDD-CESM/CDD-CESM/Subtracted images of CDD-CESM'\n",
    "json_dir = 'CDD-CESM/json_output'\n",
    "excel_path = 'processed_metadata.csv'\n",
    "\n",
    "# 📂 Exemple: structure d'un échantillon\n",
    "# images/patient123_Left_LE.png, images/patient123_Left_SUB.png, ...\n",
    "# json_reports/patient123.json\n",
    "\n",
    "def load_image_stack(patient_id):\n",
    "    paths = [\n",
    "        f\"{image_dir_le}/P{patient_id}_L_DM_CC.jpg\",\n",
    "        f\"{image_dir_le}/P{patient_id}_L_DM_MLO.jpg\",\n",
    "        f\"{image_dir_sub}/P{patient_id}_L_CM_CC.jpg\",\n",
    "        f\"{image_dir_sub}/P{patient_id}_L_CM_MLO.jpg\",\n",
    "        f\"{image_dir_le}/P{patient_id}_R_DM_CC.jpg\",\n",
    "        f\"{image_dir_le}/P{patient_id}_R_DM_MLO.jpg\",\n",
    "        f\"{image_dir_sub}/P{patient_id}_R_CM_CC.jpg\",\n",
    "        f\"{image_dir_sub}/P{patient_id}_R_CM_MLO.jpg\",\n",
    "    ]\n",
    "    imgs = [np.array(Image.open(p).resize((224, 224))) for p in paths]\n",
    "    return np.stack(imgs, axis=-1) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔢 Charger Excel (métadonnées)\n",
    "meta_df = pd.read_csv(excel_path)\n",
    "\n",
    "# 🧹 Nettoyage minimal\n",
    "meta_df = meta_df.dropna(subset=['Patient_ID', 'Pathology Classification/ Follow up'])\n",
    "meta_df['Patient_ID'] = meta_df['Patient_ID'].astype(str)\n",
    "\n",
    "# ⚙️ Encodage et normalisation\n",
    "numerical = meta_df.select_dtypes(include=['float', 'int']).columns.tolist()\n",
    "categorical = meta_df.select_dtypes(include=['object']).drop(columns=['Patient_ID', 'Pathology Classification/ Follow up']).columns.tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "meta_num = scaler.fit_transform(meta_df[numerical])\n",
    "meta_cat = encoder.fit_transform(meta_df[categorical])\n",
    "meta_features = np.concatenate([meta_num, meta_cat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Étape 1 : charger les rapports texte pour TOUS les patients (ordre = meta_df)\n",
    "texts = []\n",
    "for pid in meta_df['Patient_ID']:\n",
    "    path = os.path.join(json_dir, f\"P{pid}.json\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                d = json.load(f)\n",
    "                flat_text = []\n",
    "                for v in d.values():\n",
    "                    flat_text.extend(map(str, v) if isinstance(v, list) else [str(v)])\n",
    "                texts.append(\" \".join(flat_text))\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur JSON {pid}: {e}\")\n",
    "                texts.append(\"\")\n",
    "    else:\n",
    "        texts.append(\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Étape 2 : échantillonner 10 % des patients\n",
    "sampled_df = meta_df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# get sampled patient IDs\n",
    "sampled_patient_ids = sampled_df['Patient_ID'].astype(str).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Étape 3 : charger les images valides et enregistrer les PID correspondants\n",
    "images = []\n",
    "valid_labels = []\n",
    "valid_pids = []\n",
    "\n",
    "for pid, label in zip(sampled_df['Patient_ID'], sampled_df['Pathology Classification/ Follow up']):\n",
    "    try:\n",
    "        img = load_image_stack(pid)\n",
    "        images.append(img)\n",
    "        valid_labels.append(label)\n",
    "        valid_pids.append(pid)\n",
    "    except:\n",
    "        continue  # Ignore les erreurs (images manquantes etc.)\n",
    "\n",
    "images = np.array(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated Images: 122 out of 201 samples\n",
      "Validated Labels: 122 out of 201 samples\n",
      "Validated Texts: 122 out of 201 samples\n",
      "Validated Metadata Features: 122 out of 201 samples\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have lists or arrays for images, labels, texts, and metadata features\n",
    "# and a DataFrame meta_df with a 'Patient_ID' column\n",
    "\n",
    "# Extract patient IDs from each dataset\n",
    "image_patient_ids = set([pid for pid, img in zip(sampled_patient_ids, images)])\n",
    "text_patient_ids = set([pid for pid, text in zip(sampled_patient_ids, texts)])\n",
    "metadata_patient_ids = set(meta_df['Patient_ID'].unique())\n",
    "\n",
    "# Find the intersection of all patient IDs\n",
    "common_patient_ids = list(image_patient_ids.intersection(text_patient_ids).intersection(metadata_patient_ids))\n",
    "\n",
    "# Create a dictionary to map patient IDs to their corresponding images, labels, and texts\n",
    "image_dict = {pid: img for pid, img in zip(sampled_patient_ids, images) if pid in common_patient_ids}\n",
    "label_dict = {pid: label for pid, label in zip(sampled_patient_ids, valid_labels) if pid in common_patient_ids}\n",
    "text_dict = {pid: text for pid, text in zip(sampled_patient_ids, texts) if pid in common_patient_ids}\n",
    "meta_dict = {pid: meta_features[i] for i, pid in enumerate(sampled_patient_ids) if pid in common_patient_ids}\n",
    "\n",
    "# Filter metadata features for common patient IDs\n",
    "meta_features_filtered = meta_features[meta_df['Patient_ID'].isin(common_patient_ids)]\n",
    "\n",
    "# Ensure that each patient ID appears only once and corresponds correctly\n",
    "images_filtered = [image_dict[pid] for pid in common_patient_ids]\n",
    "labels_filtered = [label_dict[pid] for pid in common_patient_ids]\n",
    "texts_filtered = [text_dict[pid] for pid in common_patient_ids]\n",
    "meta_features_filtered = [meta_dict[pid] for pid in common_patient_ids]\n",
    "\n",
    "# Check the number of validated samples\n",
    "print(f\"Validated Images: {len(images_filtered)} out of {len(sampled_patient_ids)} samples\")\n",
    "print(f\"Validated Labels: {len(labels_filtered)} out of {len(sampled_patient_ids)} samples\")\n",
    "print(f\"Validated Texts: {len(texts_filtered)} out of {len(sampled_patient_ids)} samples\")\n",
    "print(f\"Validated Metadata Features: {len(meta_features_filtered)} out of {len(sampled_patient_ids)} samples\")\n",
    "\n",
    "# Ensure all filtered datasets have the same number of samples\n",
    "assert len(images_filtered) == len(labels_filtered) == len(texts_filtered) == len(meta_features_filtered), \\\n",
    "    \"Mismatch in the number of samples after filtering.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Étape 5 : vectoriser les textes (fit sur corpus réduit)\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "text_features_filtered = vectorizer.fit_transform(texts_filtered).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'select_dtypes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- Étape 6 : scaler et encoder uniquement sur meta_df_filtered\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m numerical \u001b[38;5;241m=\u001b[39m \u001b[43mmeta_features_filtered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_dtypes\u001b[49m(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      3\u001b[0m categorical \u001b[38;5;241m=\u001b[39m meta_features_filtered\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPatient_ID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPathology Classification/ Follow up\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      5\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'select_dtypes'"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Étape 6 : scaler et encoder uniquement sur meta_df_filtered\n",
    "numerical = meta_features_filtered.select_dtypes(include=['float', 'int']).columns.tolist()\n",
    "categorical = meta_features_filtered.select_dtypes(include=['object']).drop(columns=['Patient_ID', 'Pathology Classification/ Follow up']).columns.tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "meta_num_filtered = scaler.fit_transform(meta_features_filtered[numerical])\n",
    "meta_cat_filtered = encoder.fit_transform(meta_features_filtered[categorical])\n",
    "print(f\"Nombre de caractéristiques numériques : {meta_num_filtered.shape[1]}\")\n",
    "print(f\"Nombre de caractéristiques catégorielles : {meta_cat_filtered.shape[1]}\")\n",
    "meta_features_filtered = np.concatenate([meta_num_filtered, meta_cat_filtered], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Étape 7 : encoder les labels\n",
    "valid_labels_encoded, label_names = pd.factorize(valid_labels)\n",
    "labels_cat = to_categorical(valid_labels_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # --- Étape 8 : vérification finale\n",
    "# assert len(images) == meta_features_filtered.shape[0] == text_features_filtered.shape[0] == labels_cat.shape[0], \\\n",
    "#     f\"Mismatch: images={len(images)}, meta={meta_features_filtered.shape[0]}, text={text_features_filtered.shape[0]}, labels={labels_cat.shape[0]}\"\n",
    "\n",
    "# print(\"✅ Données bien alignées :\", len(images), \"exemples valides\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Définir l’architecture multimodale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrées\n",
    "img_input = Input(shape=(224, 224, 4))\n",
    "meta_input = Input(shape=(meta_features.shape[1],))\n",
    "text_input = Input(shape=(text_features.shape[1],))\n",
    "\n",
    "# Branch image\n",
    "x_img = Conv2D(32, (3, 3), activation='relu')(img_input)\n",
    "x_img = MaxPooling2D()(x_img)\n",
    "x_img = Conv2D(64, (3, 3), activation='relu')(x_img)\n",
    "x_img = GlobalAveragePooling2D()(x_img)\n",
    "\n",
    "# Branch metadata\n",
    "x_meta = Dense(64, activation='relu')(meta_input)\n",
    "\n",
    "# Branch texte (TF-IDF direct)\n",
    "x_text = Dense(64, activation='relu')(text_input)\n",
    "\n",
    "# Fusion\n",
    "x = Concatenate()([x_img, x_meta, x_text])\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(len(label_names), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[img_input, meta_input, text_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚂 Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔀 Split\n",
    "X_img_train, X_img_val, X_meta_train, X_meta_val, X_txt_train, X_txt_val, y_train, y_val = train_test_split(\n",
    "    images, meta_features_filtered, text_features_filtered, labels_cat, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# 🚀 Fit\n",
    "history = model.fit(\n",
    "    [X_img_train, X_meta_train, X_txt_train], y_train,\n",
    "    validation_data=([X_img_val, X_meta_val, X_txt_val], y_val),\n",
    "    epochs=10, batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Étape 5 : vectoriser les textes (fit sur corpus réduit)\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "text_features_filtered = vectorizer.fit_transform(texts_filtered).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Étape 6 : scaler et encoder uniquement sur meta_df_filtered\n",
    "numerical = meta_df_filtered.select_dtypes(include=['float', 'int']).columns.tolist()\n",
    "categorical = meta_df_filtered.select_dtypes(include=['object']).drop(columns=['Patient_ID', 'Pathology Classification/ Follow up']).columns.tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "meta_num_filtered = scaler.fit_transform(meta_df_filtered[numerical])\n",
    "meta_cat_filtered = encoder.fit_transform(meta_df_filtered[categorical])\n",
    "print(f\"Nombre de caractéristiques numériques : {meta_num_filtered.shape[1]}\")\n",
    "print(f\"Nombre de caractéristiques catégorielles : {meta_cat_filtered.shape[1]}\")\n",
    "meta_features_filtered = np.concatenate([meta_num_filtered, meta_cat_filtered], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Étape 7 : encoder les labels\n",
    "valid_labels_encoded, label_names = pd.factorize(valid_labels)\n",
    "labels_cat = to_categorical(valid_labels_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # --- Étape 8 : vérification finale\n",
    "# assert len(images) == meta_features_filtered.shape[0] == text_features_filtered.shape[0] == labels_cat.shape[0], \\\n",
    "#     f\"Mismatch: images={len(images)}, meta={meta_features_filtered.shape[0]}, text={text_features_filtered.shape[0]}, labels={labels_cat.shape[0]}\"\n",
    "\n",
    "# print(\"✅ Données bien alignées :\", len(images), \"exemples valides\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Définir l’architecture multimodale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrées\n",
    "img_input = Input(shape=(224, 224, 4))\n",
    "meta_input = Input(shape=(meta_features.shape[1],))\n",
    "text_input = Input(shape=(text_features.shape[1],))\n",
    "\n",
    "# Branch image\n",
    "x_img = Conv2D(32, (3, 3), activation='relu')(img_input)\n",
    "x_img = MaxPooling2D()(x_img)\n",
    "x_img = Conv2D(64, (3, 3), activation='relu')(x_img)\n",
    "x_img = GlobalAveragePooling2D()(x_img)\n",
    "\n",
    "# Branch metadata\n",
    "x_meta = Dense(64, activation='relu')(meta_input)\n",
    "\n",
    "# Branch texte (TF-IDF direct)\n",
    "x_text = Dense(64, activation='relu')(text_input)\n",
    "\n",
    "# Fusion\n",
    "x = Concatenate()([x_img, x_meta, x_text])\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(len(label_names), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[img_input, meta_input, text_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚂 Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔀 Split\n",
    "X_img_train, X_img_val, X_meta_train, X_meta_val, X_txt_train, X_txt_val, y_train, y_val = train_test_split(\n",
    "    images, meta_features_filtered, text_features_filtered, labels_cat, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# 🚀 Fit\n",
    "history = model.fit(\n",
    "    [X_img_train, X_meta_train, X_txt_train], y_train,\n",
    "    validation_data=([X_img_val, X_meta_val, X_txt_val], y_val),\n",
    "    epochs=10, batch_size=16\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
