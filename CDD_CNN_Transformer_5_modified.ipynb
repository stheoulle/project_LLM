{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ““ Notebook : Classification multimodale sÃ©quentielle CNN + Transformer\n",
    "\n",
    "# ğŸ“¦ Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout,\n",
    "    LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D,\n",
    "    TimeDistributed, Concatenate, Lambda\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from PIL import Image\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_exist(path_list):\n",
    "    return all(os.path.exists(p) for p in path_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Dossiers (Ã  adapter)\n",
    "image_dir_le = 'CDD-CESM/PKG - CDD-CESM/CDD-CESM/Low energy images of CDD-CESM'\n",
    "image_dir_sub = 'CDD-CESM/PKG - CDD-CESM/CDD-CESM/Subtracted images of CDD-CESM'\n",
    "json_dir = 'CDD-CESM/json_output'\n",
    "excel_path = 'processed_metadata.csv'\n",
    "\n",
    "X_img_train_feats_path = 'model/X_img_train_feats.npy'\n",
    "X_img_val_feats_path = 'model/X_img_val_feats.npy'\n",
    "X_img_test_feats_path = 'model/X_img_test_feats.npy'\n",
    "\n",
    "\n",
    "# --- Fonction de chargement sÃ©quentiel des 8 images (chaque image 224x224x1)\n",
    "def load_images_sequential_partial(patient_id):\n",
    "    paths = [\n",
    "        f\"{image_dir_le}/P{patient_id}_L_DM_CC.jpg\",\n",
    "        f\"{image_dir_le}/P{patient_id}_L_DM_MLO.jpg\",\n",
    "        f\"{image_dir_sub}/P{patient_id}_L_CM_CC.jpg\",\n",
    "        f\"{image_dir_sub}/P{patient_id}_L_CM_MLO.jpg\",\n",
    "        f\"{image_dir_le}/P{patient_id}_R_DM_CC.jpg\",\n",
    "        f\"{image_dir_le}/P{patient_id}_R_DM_MLO.jpg\",\n",
    "        f\"{image_dir_sub}/P{patient_id}_R_CM_CC.jpg\",\n",
    "        f\"{image_dir_sub}/P{patient_id}_R_CM_MLO.jpg\",\n",
    "    ]\n",
    "    imgs = []\n",
    "    valid_found = False\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                img = Image.open(p).convert('L').resize((224, 224))\n",
    "                arr = np.array(img) / 255.0\n",
    "                arr = arr[..., np.newaxis]\n",
    "                valid_found = True\n",
    "            except:\n",
    "                arr = np.zeros((224, 224, 1), dtype=np.float32)\n",
    "        else:\n",
    "            arr = np.zeros((224, 224, 1), dtype=np.float32)\n",
    "        imgs.append(arr)\n",
    "    return np.stack(imgs, axis=0) if valid_found else None\n",
    "\n",
    "\n",
    "# --- Chargement des mÃ©tadonnÃ©es\n",
    "meta_df = pd.read_csv(excel_path)\n",
    "meta_df = meta_df.dropna(subset=['Patient_ID', 'Pathology Classification/ Follow up'])\n",
    "meta_df['Patient_ID'] = meta_df['Patient_ID'].astype(str)\n",
    "\n",
    "# --- Chargement textes JSON\n",
    "texts = []\n",
    "for pid in meta_df['Patient_ID']:\n",
    "    path = os.path.join(json_dir, f\"P{pid}.json\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                d = json.load(f)\n",
    "                flat_text = []\n",
    "                for v in d.values():\n",
    "                    flat_text.extend(map(str, v) if isinstance(v, list) else [str(v)])\n",
    "                texts.append(\" \".join(flat_text))\n",
    "            except:\n",
    "                texts.append(\"\")\n",
    "    else:\n",
    "        texts.append(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Echantillonnage 80%\n",
    "sampled_df = meta_df.sample(frac=0.8, random_state=42)\n",
    "sampled_patient_ids = sampled_df['Patient_ID'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Chargement sÃ©quentiel des images + labels\n",
    "images = []\n",
    "valid_labels = []\n",
    "valid_pids = []\n",
    "skipped_image = 0\n",
    "\n",
    "for pid, label in zip(sampled_df['Patient_ID'], sampled_df['Pathology Classification/ Follow up']):\n",
    "    imgs_seq = load_images_sequential_partial(pid)\n",
    "    if imgs_seq is not None:\n",
    "        images.append(imgs_seq)\n",
    "        valid_labels.append(label)\n",
    "        valid_pids.append(pid)\n",
    "    else:\n",
    "        skipped_image += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Alignement des donnÃ©es\n",
    "image_patient_ids = set(valid_pids)\n",
    "text_patient_ids = set(sampled_patient_ids)\n",
    "metadata_patient_ids = set(meta_df['Patient_ID'])\n",
    "common_patient_ids = list(image_patient_ids & text_patient_ids & metadata_patient_ids)\n",
    "\n",
    "image_dict = {pid: img for pid, img in zip(valid_pids, images) if pid in common_patient_ids}\n",
    "label_dict = {pid: label for pid, label in zip(valid_pids, valid_labels) if pid in common_patient_ids}\n",
    "text_dict = {pid: text for pid, text in zip(sampled_patient_ids, texts) if pid in common_patient_ids}\n",
    "meta_dict = {pid: meta_df[meta_df['Patient_ID'] == pid].iloc[0] for pid in common_patient_ids}\n",
    "\n",
    "images_filtered = np.array([image_dict[pid] for pid in common_patient_ids])  # shape (N, 8, 224, 224, 1)\n",
    "labels_filtered = [label_dict[pid] for pid in common_patient_ids]\n",
    "texts_filtered = [text_dict[pid] for pid in common_patient_ids]\n",
    "meta_df_filtered = pd.DataFrame([meta_dict[pid] for pid in common_patient_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Textes vectorisÃ©s\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "text_features_filtered = vectorizer.fit_transform(texts_filtered).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Encodage des mÃ©tadonnÃ©es\n",
    "numerical = meta_df_filtered.select_dtypes(include=['float', 'int']).columns.tolist()\n",
    "categorical = meta_df_filtered.select_dtypes(include=['object']).drop(columns=['Patient_ID', 'Pathology Classification/ Follow up']).columns.tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "meta_num_filtered = scaler.fit_transform(meta_df_filtered[numerical])\n",
    "meta_cat_filtered = encoder.fit_transform(meta_df_filtered[categorical])\n",
    "meta_features_filtered = np.concatenate([meta_num_filtered, meta_cat_filtered], axis=1)\n",
    "\n",
    "# --- Encodage des labels\n",
    "valid_labels_encoded, label_names = pd.factorize(labels_filtered)\n",
    "labels_cat = to_categorical(valid_labels_encoded)\n",
    "\n",
    "# --- Poids de classes\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(valid_labels_encoded), y=valid_labels_encoded)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Split train/test\n",
    "X_img_temp, X_img_test, X_meta_temp, X_meta_test, X_txt_temp, X_txt_test, y_temp, y_test = train_test_split(\n",
    "    images_filtered, meta_features_filtered, text_features_filtered, labels_cat,\n",
    "    test_size=0.1, random_state=42)\n",
    "\n",
    "X_img_train, X_img_val, X_meta_train, X_meta_val, X_txt_train, X_txt_val, y_train, y_val = train_test_split(\n",
    "    X_img_temp, X_meta_temp, X_txt_temp, y_temp,\n",
    "    test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.keras import TqdmCallback\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# y_train est one-hot -> reprends les labels\n",
    "\n",
    "if features_exist([X_img_train_feats_path, X_img_val_feats_path, X_img_test_feats_path]):\n",
    "    print(\"ğŸ”„ Chargement des features extraites du modÃ¨le CNN...\")\n",
    "    X_img_train_feats = np.load(X_img_train_feats_path)\n",
    "    X_img_val_feats = np.load(X_img_val_feats_path)\n",
    "    X_img_test_feats = np.load(X_img_test_feats_path)\n",
    "\n",
    "\n",
    "    np.argmax(y_train, axis=1)\n",
    "\n",
    "    print(\"Distribution dans y_train :\", Counter(np.argmax(y_train, axis=1)))\n",
    "else:\n",
    "    print(\"ğŸš€ EntraÃ®nement du modÃ¨le CNN et extraction des features...\")\n",
    "\n",
    "    # Phase 1 : entraÃ®nement CNN\n",
    "    history_phase1 = cnn_model.fit(\n",
    "        X_img_train,\n",
    "        y_train,\n",
    "        validation_data=(X_img_val, y_val),\n",
    "        epochs=30,\n",
    "        batch_size=16,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=[TqdmCallback(verbose=1)]\n",
    "    )\n",
    "\n",
    "    # Extraction des features\n",
    "    cnn_feature_extractor = Model(inputs=img_seq_input, outputs=x_img_seq)\n",
    "    cnn_feature_extractor.trainable = False\n",
    "\n",
    "    X_img_train_feats = cnn_feature_extractor.predict(X_img_train)\n",
    "    X_img_val_feats = cnn_feature_extractor.predict(X_img_val)\n",
    "    X_img_test_feats = cnn_feature_extractor.predict(X_img_test)\n",
    "\n",
    "    # Sauvegarde pour les prochains runs\n",
    "    np.save(X_img_train_feats_path, X_img_train_feats)\n",
    "    np.save(X_img_val_feats_path, X_img_val_feats)\n",
    "    np.save(X_img_test_feats_path, X_img_test_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Dropout, Concatenate, GlobalAveragePooling1D, Lambda,\n",
    "    TimeDistributed, GlobalAveragePooling2D\n",
    ")\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "# --- Focal loss sans clip() ---\n",
    "def focal_loss(gamma=2., alpha=0.25):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
    "        weight = alpha * tf.pow(1 - y_pred, gamma)\n",
    "        loss = weight * cross_entropy\n",
    "        return tf.reduce_mean(tf.reduce_sum(loss, axis=-1))\n",
    "    return loss_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- TransformerBlock ---\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- CrÃ©ation de l'encodeur CNN RadImageNet\n",
    "def create_radimagenet_encoder():\n",
    "    base_model = DenseNet121(\n",
    "        include_top=False,\n",
    "        weights=None,\n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    base_model.load_weights(\"weights/RadImageNet-DenseNet121_notop.h5\")\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = Input(shape=(224, 224, 1))\n",
    "    x = Lambda(lambda img: tf.image.grayscale_to_rgb(img))(inputs)\n",
    "    x = preprocess_input(x)\n",
    "    x = base_model(x, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(32, activation='relu')(x)  # <--- rÃ©duction de la taille\n",
    "    return Model(inputs, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- ModÃ¨le Multimodal Phase 2\n",
    "cnn_encoder = create_radimagenet_encoder()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- EntrÃ©e image (features extraits par CNN, ex: RadImageNet)\n",
    "img_feat_input = Input(shape=(X_img_train_feats.shape[1],), name='cnn_img_feat')\n",
    "\n",
    "# --- EntrÃ©e mÃ©tadonnÃ©es\n",
    "meta_input = Input(shape=(X_meta_train.shape[1],), name='meta_input')\n",
    "x_meta = Dense(32, activation='relu')(meta_input)\n",
    "\n",
    "# --- EntrÃ©e texte\n",
    "text_input = Input(shape=(X_txt_train.shape[1],), name='text_input')\n",
    "x_text = Dense(32, activation='relu')(text_input)\n",
    "\n",
    "# --- Fusion des 3 modalitÃ©s\n",
    "x_concat = Concatenate(axis=-1)([img_feat_input, x_meta, x_text])  # (None, total_dim)\n",
    "x = Lambda(lambda x: tf.expand_dims(x, axis=1))(x_concat)\n",
    "\n",
    "# --- Transformer multimodal\n",
    "x = TransformerBlock(embed_dim=x_concat.shape[-1], num_heads=4, ff_dim=128)(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "# --- Classification\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(len(label_names), activation='softmax')(x)\n",
    "\n",
    "# --- Compilation\n",
    "model_phase2 = Model(inputs=[img_feat_input, meta_input, text_input], outputs=output)\n",
    "model_phase2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_phase2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Callbacks\n",
    "early_stopping = EarlyStopping(monitor='accuracy', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(\"best_model_phase2.h5\", monitor='val_loss', save_best_only=True)\n",
    "logger = CSVLogger(\"training_log_phase2.csv\")\n",
    "\n",
    "# --- EntraÃ®nement\n",
    "print(X_img_train_feats.shape)  # doit Ãªtre (N, 32) si modÃ¨le l'attend\n",
    "\n",
    "history_phase2 = model_phase2.fit(\n",
    "    [X_img_train_feats, X_meta_train, X_txt_train], y_train,\n",
    "    validation_data=([X_img_val_feats, X_meta_val, X_txt_val], y_val),\n",
    "    epochs=30,\n",
    "    batch_size=16,\n",
    "    class_weight=class_weight_dict,\n",
    "    # callbacks=[early_stopping, checkpoint, logger]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- ModÃ¨le multimodal complet (Phase 3)\n",
    "# img_seq_input = Input(shape=(8, 224, 224, 1), name='img_seq_input')\n",
    "# meta_input = Input(shape=(X_meta_train.shape[1],), name='meta_input')\n",
    "# text_input = Input(shape=(X_txt_train.shape[1],), name='text_input')\n",
    "\n",
    "# cnn_encoder_finetune = create_finetune_cnn_encoder()\n",
    "# x_img_seq = TimeDistributed(cnn_encoder_finetune)(img_seq_input)\n",
    "# x_img_seq = TransformerBlock(embed_dim=64, num_heads=4, ff_dim=128)(x_img_seq)\n",
    "# x_img_seq = GlobalAveragePooling1D()(x_img_seq)\n",
    "\n",
    "# x_meta = Dense(64, activation='relu')(meta_input)\n",
    "# x_text = Dense(64, activation='relu')(text_input)\n",
    "\n",
    "# x = Concatenate()([x_img_seq, x_meta, x_text])\n",
    "# x = Lambda(lambda x: tf.expand_dims(x, axis=1))(x)\n",
    "# x = TransformerBlock(embed_dim=192, num_heads=4, ff_dim=256)(x)\n",
    "# x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "# x = Dense(128, activation='relu')(x)\n",
    "# x = Dropout(0.3)(x)\n",
    "# x = Dense(64, activation='relu')(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "# output = Dense(len(label_names), activation='softmax')(x)\n",
    "\n",
    "# model_phase3 = Model(inputs=[img_seq_input, meta_input, text_input], outputs=output)\n",
    "# model_phase3.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(1e-6),\n",
    "#     loss='categorical_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# model_phase3.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Ã‰valuation sur test\n",
    "# PrÃ©dictions\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# PrÃ©dictions\n",
    "y_pred_phase2 = model_phase2.predict([X_img_test_feats, X_meta_test, X_txt_test])\n",
    "y_pred_classes = np.argmax(y_pred_phase2, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Scores\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=label_names))\n",
    "print(\"F1-score:\", f1_score(y_true_classes, y_pred_classes, average='weighted'))\n",
    "print(\"Accuracy:\", accuracy_score(y_true_classes, y_pred_classes))\n",
    "\n",
    "# ROC AUC\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_pred_phase2, average=None, multi_class='ovr'))\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "ConfusionMatrixDisplay(cm, display_labels=label_names).plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - Multimodal Transformer\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history, title=\"Training History\"):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs_range = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title(f'{title} - Accuracy')\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f'{title} - Loss')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Appel pour phase 2\n",
    "# plot_training_history(history_phase1, title=\"CNN Phase 1\")\n",
    "plot_training_history(history_phase2, title=\"Multimodal Phase 2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 1. VÃ©rifier la distribution des classes\n",
    "def check_class_distribution(y_train, y_val, y_test):\n",
    "    print(\"Distribution classes y_train :\", Counter(np.argmax(y_train, axis=1)))\n",
    "    print(\"Distribution classes y_val   :\", Counter(np.argmax(y_val, axis=1)))\n",
    "    print(\"Distribution classes y_test  :\", Counter(np.argmax(y_test, axis=1)))\n",
    "\n",
    "# 2. VÃ©rifier les formes des donnÃ©es\n",
    "def check_data_shapes(*arrays):\n",
    "    for arr in arrays:\n",
    "        print(arr.shape)\n",
    "\n",
    "# 3. Compiler avec une loss classique (pour tester)\n",
    "def compile_model_for_debug(model):\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    print(\"ModÃ¨le compilÃ© avec categorical_crossentropy et lr=1e-4\")\n",
    "\n",
    "# 4. EntraÃ®ner un epoch rapide\n",
    "def quick_train(model, X_train, y_train, epochs=1):\n",
    "    history = model.fit(X_train, y_train, batch_size=32, epochs=epochs, validation_split=0.1)\n",
    "    return history\n",
    "\n",
    "# 5. Faire des prÃ©dictions et analyser\n",
    "def analyze_predictions(model, X_val):\n",
    "    y_pred = model.predict(X_val)\n",
    "    pred_classes = np.argmax(y_pred, axis=1)\n",
    "    unique, counts = np.unique(pred_classes, return_counts=True)\n",
    "    print(\"PrÃ©dictions classes et leurs comptes sur la validation :\")\n",
    "    for c, count in zip(unique, counts):\n",
    "        print(f\"Classe {c} : {count} prÃ©dictions\")\n",
    "    return pred_classes\n",
    "\n",
    "# Exemple d'utilisation (adapter les variables avec tes donnÃ©es et modÃ¨le)\n",
    "\n",
    "check_class_distribution(y_train, y_val, y_test)\n",
    "check_data_shapes(X_img_train_feats, X_meta_train, X_txt_train, y_train)\n",
    "compile_model_for_debug(model_phase2)\n",
    "quick_train(model_phase2, [X_img_train_feats, X_meta_train, X_txt_train], y_train, epochs=1)\n",
    "analyze_predictions(model_phase2, [X_img_val_feats, X_meta_val, X_txt_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_train[:5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
