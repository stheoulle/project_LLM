from utils.preproc import prepare_data
from utils.fusion import multimodal_fusion_model
from utils.metrics import evaluate_model, plot_metrics
from models.cnn_backbones import load_cnn_model
from models.trained import train_model
import torch

def define_steps(modality_choice, mri_types=None):
    data = prepare_data(modality_choice, mri_types)
    return data

def get_input_shape_from_sample(image):
    if isinstance(image, list):
        if len(image) == 0:
            raise ValueError("Le premier √©l√©ment de data['images'] est une liste vide.")
        image = image[0]
    if not isinstance(image, torch.Tensor):
        image = torch.tensor(image)
    return image.shape

def initialize_image_model(model_choice, data):
    if 'images' not in data or len(data['images']) == 0:
        raise ValueError("Aucune image n'a √©t√© trouv√©e dans les donn√©es.")
    print(f"üß™ data['images'] contient {len(data['images'])} √©l√©ments")
    input_shape = get_input_shape_from_sample(data['images'][0])
    print(f"üìê input_shape d√©tect√©e : {input_shape}")
    return load_cnn_model(model_choice, input_shape)

def run_pipeline(modality_choice, model_choice, mri_types=None, llm_context=None, extra_inputs=None):
    """
    Run the training pipeline. Accepts optional llm_context (assistant suggestion text or dict)
    and extra_inputs (e.g., uploaded files). These are currently logged and returned but not
    used directly by the training routines. Backward compatible with previous signature.
    """
    print(f"Mod√®le choisi : {model_choice}")
    print(f"Modalit√©s choisies : {modality_choice}")
    if mri_types:
        print(f"Types de MRI s√©lectionn√©s : {mri_types}")
    if llm_context:
        print(f"LLM context / suggestion: {llm_context}")
    if extra_inputs:
        print(f"Extra inputs provided: {list(extra_inputs.keys()) if isinstance(extra_inputs, dict) else type(extra_inputs)}")

    # üß© Pr√©paration des donn√©es
    data = define_steps(modality_choice, mri_types)

    # Phase 1 ‚Äî entra√Ænement du mod√®le image only
    image_model = initialize_image_model(model_choice, data)
    history = train_model(image_model, data)
    results = evaluate_model(image_model, data)
    try:
        plot_metrics(history, results)
    except Exception:
        print("plot_metrics failed or running headless; continuing.")
    print("Phase 1 termin√©e : mod√®le image entra√Æn√© et √©valu√©.")
    print(f"D√©but de la phase 2 avec le mod√®le {model_choice} et les donn√©es pr√©par√©es.")

    # Phase 2 ‚Äî entra√Ænement du mod√®le multimodal (avec le mod√®le image d√©j√† entra√Æn√©)
    fusion_model = multimodal_fusion_model(image_model, data)
    print(f"Mod√®le de fusion multimodal cr√©√© avec {model_choice} comme backbone.")
    fusion_history = train_model(fusion_model, data)
    print("Phase 2 termin√©e : mod√®le multimodal entra√Æn√©.")
    fusion_results = evaluate_model(fusion_model, data)
    print("√âvaluation du mod√®le multimodal termin√©e.")
    try:
        plot_metrics(fusion_history, fusion_results)
    except Exception:
        print("plot_metrics failed for fusion model; continuing.")
    print("Pipeline complet : mod√®le multimodal entra√Æn√© et √©valu√©.")

    # Return a summary dictionary so callers (UI/CLI) can display or log info
    return {
        'phase1_history': history,
        'phase1_results': results,
        'fusion_history': fusion_history,
        'fusion_results': fusion_results,
        'llm_context': llm_context,
        'extra_inputs': extra_inputs
    }

